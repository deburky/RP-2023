{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabNet\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    This is a pyTorch implementation of Tabnet (Arik, S. O., & Pfister, T. 2019) by <a href=\"https://github.com/dreamquark-ai)\">DreamQuark</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=50000, n_features=15, n_classes=2, n_informative=5, random_state=42\n",
    ")\n",
    "\n",
    "X, y = pd.DataFrame(X), pd.DataFrame(y)\n",
    "\n",
    "X.columns = [f\"feature_{i+1}\" for i in range(len(X.columns))]\n",
    "\n",
    "ix_train, ix_test = train_test_split(X.index, stratify=y, random_state=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GiniScore(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"gini\"\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        auc = roc_auc_score(y_true, y_score[:, 1])\n",
    "        return max(2 * auc - 1, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/risk-practitioner-ebook-NcspVTUP-py3.10/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "batch_size = round(0.10 * len(X.loc[ix_train]))\n",
    "clf = TabNetClassifier(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,\n",
    "    scheduler_params={\n",
    "        \"is_batch_level\": True,\n",
    "        \"max_lr\": 5e-2,\n",
    "        \"steps_per_epoch\": int(X.loc[ix_train].shape[0] / batch_size) + 1,\n",
    "        \"epochs\": max_epochs,\n",
    "    },\n",
    "    mask_type=\"entmax\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.72843 | train_logloss: 0.69951 | train_gini: 0.36982 | test_logloss: 0.69944 | test_gini: 0.37703 |  0:00:03s\n",
      "epoch 1  | loss: 0.60558 | train_logloss: 0.57118 | train_gini: 0.55522 | test_logloss: 0.5682  | test_gini: 0.56746 |  0:00:06s\n",
      "epoch 2  | loss: 0.54865 | train_logloss: 0.50776 | train_gini: 0.65734 | test_logloss: 0.50583 | test_gini: 0.66613 |  0:00:09s\n",
      "epoch 3  | loss: 0.50686 | train_logloss: 0.46785 | train_gini: 0.71664 | test_logloss: 0.46232 | test_gini: 0.72472 |  0:00:12s\n",
      "epoch 4  | loss: 0.47736 | train_logloss: 0.43841 | train_gini: 0.75475 | test_logloss: 0.43204 | test_gini: 0.76403 |  0:00:15s\n",
      "epoch 5  | loss: 0.44659 | train_logloss: 0.40937 | train_gini: 0.78939 | test_logloss: 0.40182 | test_gini: 0.79961 |  0:00:18s\n",
      "epoch 6  | loss: 0.419   | train_logloss: 0.3838  | train_gini: 0.81762 | test_logloss: 0.37717 | test_gini: 0.82615 |  0:00:22s\n",
      "epoch 7  | loss: 0.39237 | train_logloss: 0.36304 | train_gini: 0.83837 | test_logloss: 0.3584  | test_gini: 0.84493 |  0:00:25s\n",
      "epoch 8  | loss: 0.37067 | train_logloss: 0.34365 | train_gini: 0.85592 | test_logloss: 0.341   | test_gini: 0.86041 |  0:00:28s\n",
      "epoch 9  | loss: 0.3493  | train_logloss: 0.327   | train_gini: 0.87031 | test_logloss: 0.32593 | test_gini: 0.87321 |  0:00:31s\n",
      "epoch 10 | loss: 0.33617 | train_logloss: 0.30814 | train_gini: 0.88638 | test_logloss: 0.30851 | test_gini: 0.88719 |  0:00:34s\n",
      "epoch 11 | loss: 0.31625 | train_logloss: 0.29188 | train_gini: 0.8997  | test_logloss: 0.29363 | test_gini: 0.89853 |  0:00:37s\n",
      "epoch 12 | loss: 0.30028 | train_logloss: 0.27471 | train_gini: 0.91155 | test_logloss: 0.27766 | test_gini: 0.90963 |  0:00:41s\n",
      "epoch 13 | loss: 0.27936 | train_logloss: 0.26054 | train_gini: 0.92094 | test_logloss: 0.26316 | test_gini: 0.9197  |  0:00:45s\n",
      "epoch 14 | loss: 0.26941 | train_logloss: 0.24573 | train_gini: 0.92986 | test_logloss: 0.24958 | test_gini: 0.92783 |  0:00:48s\n",
      "epoch 15 | loss: 0.25454 | train_logloss: 0.23231 | train_gini: 0.93694 | test_logloss: 0.23713 | test_gini: 0.93471 |  0:00:51s\n",
      "epoch 16 | loss: 0.24335 | train_logloss: 0.21988 | train_gini: 0.9432  | test_logloss: 0.22453 | test_gini: 0.94108 |  0:00:55s\n",
      "epoch 17 | loss: 0.23444 | train_logloss: 0.20714 | train_gini: 0.94936 | test_logloss: 0.21384 | test_gini: 0.94603 |  0:00:58s\n",
      "epoch 18 | loss: 0.22159 | train_logloss: 0.19731 | train_gini: 0.95398 | test_logloss: 0.20407 | test_gini: 0.95026 |  0:01:02s\n",
      "epoch 19 | loss: 0.21177 | train_logloss: 0.18779 | train_gini: 0.95784 | test_logloss: 0.19429 | test_gini: 0.95422 |  0:01:05s\n",
      "epoch 20 | loss: 0.20712 | train_logloss: 0.18069 | train_gini: 0.96036 | test_logloss: 0.18705 | test_gini: 0.95675 |  0:01:08s\n",
      "epoch 21 | loss: 0.19632 | train_logloss: 0.1739  | train_gini: 0.96289 | test_logloss: 0.17905 | test_gini: 0.96002 |  0:01:12s\n",
      "epoch 22 | loss: 0.1926  | train_logloss: 0.16838 | train_gini: 0.96485 | test_logloss: 0.17393 | test_gini: 0.96193 |  0:01:15s\n",
      "epoch 23 | loss: 0.18361 | train_logloss: 0.16286 | train_gini: 0.96704 | test_logloss: 0.16845 | test_gini: 0.96444 |  0:01:18s\n",
      "epoch 24 | loss: 0.18179 | train_logloss: 0.15855 | train_gini: 0.96834 | test_logloss: 0.16309 | test_gini: 0.96618 |  0:01:22s\n",
      "epoch 25 | loss: 0.17338 | train_logloss: 0.1546  | train_gini: 0.96957 | test_logloss: 0.15942 | test_gini: 0.96713 |  0:01:25s\n",
      "epoch 26 | loss: 0.1654  | train_logloss: 0.15049 | train_gini: 0.97089 | test_logloss: 0.15515 | test_gini: 0.96858 |  0:01:28s\n",
      "epoch 27 | loss: 0.16682 | train_logloss: 0.14726 | train_gini: 0.97202 | test_logloss: 0.15245 | test_gini: 0.96925 |  0:01:31s\n",
      "epoch 28 | loss: 0.16691 | train_logloss: 0.1454  | train_gini: 0.97242 | test_logloss: 0.15098 | test_gini: 0.96948 |  0:01:35s\n",
      "epoch 29 | loss: 0.1584  | train_logloss: 0.1432  | train_gini: 0.97341 | test_logloss: 0.14998 | test_gini: 0.97014 |  0:01:38s\n",
      "epoch 30 | loss: 0.15784 | train_logloss: 0.14012 | train_gini: 0.97434 | test_logloss: 0.14545 | test_gini: 0.97137 |  0:01:41s\n",
      "epoch 31 | loss: 0.14957 | train_logloss: 0.13832 | train_gini: 0.97481 | test_logloss: 0.1438  | test_gini: 0.97188 |  0:01:44s\n",
      "epoch 32 | loss: 0.15002 | train_logloss: 0.13659 | train_gini: 0.97534 | test_logloss: 0.1422  | test_gini: 0.97249 |  0:01:47s\n",
      "epoch 33 | loss: 0.15564 | train_logloss: 0.13548 | train_gini: 0.97571 | test_logloss: 0.14285 | test_gini: 0.97223 |  0:01:50s\n",
      "epoch 34 | loss: 0.14774 | train_logloss: 0.13269 | train_gini: 0.97659 | test_logloss: 0.13902 | test_gini: 0.97339 |  0:01:54s\n",
      "epoch 35 | loss: 0.14647 | train_logloss: 0.13126 | train_gini: 0.97701 | test_logloss: 0.13874 | test_gini: 0.9734  |  0:01:57s\n",
      "epoch 36 | loss: 0.14431 | train_logloss: 0.1302  | train_gini: 0.97719 | test_logloss: 0.13659 | test_gini: 0.97392 |  0:02:00s\n",
      "epoch 37 | loss: 0.14394 | train_logloss: 0.12839 | train_gini: 0.9778  | test_logloss: 0.1339  | test_gini: 0.97494 |  0:02:04s\n",
      "epoch 38 | loss: 0.14519 | train_logloss: 0.12697 | train_gini: 0.97837 | test_logloss: 0.13303 | test_gini: 0.97546 |  0:02:07s\n",
      "epoch 39 | loss: 0.14045 | train_logloss: 0.12553 | train_gini: 0.97848 | test_logloss: 0.13164 | test_gini: 0.97542 |  0:02:10s\n",
      "epoch 40 | loss: 0.14314 | train_logloss: 0.12385 | train_gini: 0.9789  | test_logloss: 0.13048 | test_gini: 0.97564 |  0:02:13s\n",
      "epoch 41 | loss: 0.14054 | train_logloss: 0.12264 | train_gini: 0.97934 | test_logloss: 0.13055 | test_gini: 0.97566 |  0:02:17s\n",
      "epoch 42 | loss: 0.13969 | train_logloss: 0.12265 | train_gini: 0.97932 | test_logloss: 0.12984 | test_gini: 0.97603 |  0:02:20s\n",
      "epoch 43 | loss: 0.1325  | train_logloss: 0.12073 | train_gini: 0.98002 | test_logloss: 0.12825 | test_gini: 0.97647 |  0:02:23s\n",
      "epoch 44 | loss: 0.13334 | train_logloss: 0.11998 | train_gini: 0.98012 | test_logloss: 0.12758 | test_gini: 0.9767  |  0:02:26s\n",
      "epoch 45 | loss: 0.13415 | train_logloss: 0.11854 | train_gini: 0.98027 | test_logloss: 0.1263  | test_gini: 0.97659 |  0:02:29s\n",
      "epoch 46 | loss: 0.13391 | train_logloss: 0.11763 | train_gini: 0.98035 | test_logloss: 0.12521 | test_gini: 0.97686 |  0:02:33s\n",
      "epoch 47 | loss: 0.12983 | train_logloss: 0.11776 | train_gini: 0.98035 | test_logloss: 0.12609 | test_gini: 0.97682 |  0:02:36s\n",
      "epoch 48 | loss: 0.12903 | train_logloss: 0.11678 | train_gini: 0.98085 | test_logloss: 0.12513 | test_gini: 0.97706 |  0:02:39s\n",
      "epoch 49 | loss: 0.13373 | train_logloss: 0.11659 | train_gini: 0.98081 | test_logloss: 0.12655 | test_gini: 0.97637 |  0:02:42s\n",
      "epoch 50 | loss: 0.12921 | train_logloss: 0.11574 | train_gini: 0.98115 | test_logloss: 0.12387 | test_gini: 0.97728 |  0:02:45s\n",
      "epoch 51 | loss: 0.1291  | train_logloss: 0.11461 | train_gini: 0.98134 | test_logloss: 0.12383 | test_gini: 0.9773  |  0:02:49s\n",
      "epoch 52 | loss: 0.124   | train_logloss: 0.11355 | train_gini: 0.98162 | test_logloss: 0.12285 | test_gini: 0.97747 |  0:02:52s\n",
      "epoch 53 | loss: 0.12357 | train_logloss: 0.11361 | train_gini: 0.98155 | test_logloss: 0.12443 | test_gini: 0.97711 |  0:02:55s\n",
      "epoch 54 | loss: 0.12677 | train_logloss: 0.11231 | train_gini: 0.98209 | test_logloss: 0.12153 | test_gini: 0.97792 |  0:02:58s\n",
      "epoch 55 | loss: 0.12468 | train_logloss: 0.11166 | train_gini: 0.98216 | test_logloss: 0.12045 | test_gini: 0.97837 |  0:03:02s\n",
      "epoch 56 | loss: 0.1228  | train_logloss: 0.11132 | train_gini: 0.9822  | test_logloss: 0.12091 | test_gini: 0.9782  |  0:03:05s\n",
      "epoch 57 | loss: 0.12196 | train_logloss: 0.11084 | train_gini: 0.98226 | test_logloss: 0.1205  | test_gini: 0.97848 |  0:03:08s\n",
      "epoch 58 | loss: 0.11996 | train_logloss: 0.111   | train_gini: 0.98221 | test_logloss: 0.11995 | test_gini: 0.97839 |  0:03:12s\n",
      "epoch 59 | loss: 0.12504 | train_logloss: 0.1104  | train_gini: 0.98247 | test_logloss: 0.12089 | test_gini: 0.97817 |  0:03:15s\n",
      "epoch 60 | loss: 0.12364 | train_logloss: 0.1101  | train_gini: 0.9828  | test_logloss: 0.12008 | test_gini: 0.97847 |  0:03:18s\n",
      "epoch 61 | loss: 0.1236  | train_logloss: 0.10932 | train_gini: 0.98287 | test_logloss: 0.11955 | test_gini: 0.97838 |  0:03:21s\n",
      "epoch 62 | loss: 0.12238 | train_logloss: 0.10917 | train_gini: 0.98286 | test_logloss: 0.12113 | test_gini: 0.97794 |  0:03:24s\n",
      "\n",
      "Early stopping occurred at epoch 62 with best_epoch = 57 and best_test_gini = 0.97848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deburky/Library/Caches/pypoetry/virtualenvs/risk-practitioner-ebook-NcspVTUP-py3.10/lib/python3.10/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Gini score: 96.45%\n",
      "Test Gini score: 95.70%\n"
     ]
    }
   ],
   "source": [
    "clf.fit(\n",
    "    X_train=X.loc[ix_train].values,\n",
    "    y_train=y.loc[ix_train].values.ravel(),\n",
    "    eval_set=[\n",
    "        (X.loc[ix_train].values, y.loc[ix_train].values.ravel()),\n",
    "        (X.loc[ix_test].values, y.loc[ix_test].values.ravel()),\n",
    "    ],\n",
    "    eval_name=[\"train\", \"test\"],\n",
    "    eval_metric=[\"logloss\", GiniScore],\n",
    "    max_epochs=max_epochs,\n",
    "    patience=5,\n",
    "    batch_size=batch_size,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    weights=1,\n",
    "    drop_last=False,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    ")\n",
    "\n",
    "# Make predictions using the model with two tree iterations\n",
    "predictions_trn = clf.predict_proba(X.loc[ix_train].to_numpy())[:, 1]\n",
    "predictions_tst = clf.predict_proba(X.loc[ix_test].to_numpy())[:, 1]\n",
    "\n",
    "gini_trn = roc_auc_score(y.loc[ix_train], predictions_trn) * 2 - 1\n",
    "gini_tst = roc_auc_score(y.loc[ix_test], predictions_tst) * 2 - 1\n",
    "\n",
    "print(f\"Train Gini score: {gini_trn:.2%}\" f\"Test Gini score: {gini_tst:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk-practitioner-ebook-NcspVTUP-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
